# GENERAL
#seed=http://folk.ntnu.no/alexanno/skole/
seed=http://telenor.com
#seed=http://www.reddit.com/r/spaceporn
# max pages to analyze
max_pages=10000
# max amount of worker threads
max_workers=1
# max amount of file worker threads (ie. simultaneous downloading of non-html files like pdf, images etc)
max_file_workers=10
# interval at which the cache is dumped from memory and written to the database
cache_interval=1000

# cache parent nodes, this eats a lot of memory! suitable for debugging
cache_ancestors=true
# only search through one domain, ie. the seed domain
single_domain=false
# include searching img-tags 
# note: this does not exclude images in "a" tags
include_images=true

# DOWNLOAD & STORAGE
# stores the output in a database
store_content=true
# which filetypes to download, regex is possible
# note: you do not want to have than one crawler worker with this enabled unless you want terabytes of images
#download_filetype=png|jpg|jpeg
download_filetype=png
# folder to store downloaded files
download_location=files

# single instance can run with no GUI, however, it will not be able to resume a failed instance
enable_gui=true

# DATABASE 
jdbcDriver=org.sqlite.JDBC
url=jdbc:sqlite:crawlie.db
title=crawlie
table=title,url,domain,source,priority
user=
password=