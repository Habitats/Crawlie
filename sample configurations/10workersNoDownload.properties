# GENERAL
seed=http://telenor.com
# max pages to analyze
max_pages=2000
# max amount of worker threads
max_workers=10
# max amount of file worker threads (ie. simultaneous downloading of non-html files like pdf, images etc)
max_file_workers=10
# interval at which the cache is dumped from memory and written to the database
cache_interval=1000

# cache parent nodes, this eats a lot of memory! suitable for debugging
cache_ancestors=false
# only search through one domain, ie. the seed domain
single_domain=false
# include searching img-tags 
# note: this does not exclude images in "a" tags
include_images=false

# DOWNLOAD & STORAGE
# stores the output in a database
store_content=false
# which filetypes to download, regex is possible
# note: you do not want to have than one crawler worker with this enabled unless you want terabytes of images
download_filetype=
# folder to store downloaded files
download_location=files

# simple priority/avoidance filter. for more advanced priority/heuristic functionality, see the Priority class
prioritize=telenor
# avoid
avoid=facebook.com

# single instance can run with no GUI, however, it will not be able to resume a failed instance
enable_gui=true

# DATABASE 
jdbcDriver=org.sqlite.JDBC
url=jdbc:sqlite:crawlie.db
title=crawlie
table=title,url,domain,source,priority
user=
password=